{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install crucial libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f5a09933026f90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T08:32:11.371330Z",
     "start_time": "2025-05-22T08:32:11.363270Z"
    }
   },
   "id": "e7a5374990a211da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b8698de8dbe84ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "486238796eb913ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## **Part 0: Choose and Test Your Topic Without a Knowledge Base**\n",
    "\n",
    "Before you load any external documents, you must **verify that your chosen topic needs a knowledge base** to improve answers. This ensures your RAG system solves a real gap in the model’s knowledge.\n",
    "\n",
    "###  **Steps:**\n",
    "\n",
    "1. **Choose a Topic (Tentative)**\n",
    "\n",
    "   * Pick a topic from 2024 or 2025 that you think is recent or under-documented.\n",
    "   * Example topics:\n",
    "\n",
    "     * A political decision (e.g., \"European Union climate laws in 2024\")\n",
    "     * A cultural trend (e.g., \"Music trends in early 2025\")\n",
    "\n",
    "2. **Formulate Question**\n",
    "\n",
    "   * Write down one factual, clear question about the topic.\n",
    "   * Aim for question that require up-to-date or specific knowledge.\n",
    "\n",
    "3. **Query the Model Directly**\n",
    "\n",
    "   * Use your LLM pipeline (without RAG) to ask this question.\n",
    "   * Collect the model’s answer and evaluate their quality:\n",
    "\n",
    "     * Are the answers incomplete?\n",
    "     * Are they outdated?\n",
    "     * Are they confident but wrong?\n",
    "     * Do they say *\"I don’t know\"*?\n",
    "\n",
    "---\n",
    "\n",
    "Why This Matters:\n",
    "\n",
    "This step ensures your RAG project is solving a **real information gap**, not just repeating what the model already knows.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f35c05faf6b52cbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-22T08:32:24.960957Z"
    }
   },
   "id": "c247de87f1159695"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f0b2c2964b804dbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Part 1: Load a Custom PDF Knowledge Base**\n",
    "\n",
    "Find blog posts or wikipedia page with your topic and save information about it to a PDF file, and load it using `PyPDFLoader`. You may use other loaders not only pdf, but pdf loader is exactly the same as we used during lab.\n",
    "\n",
    "- Find informative content on your topic (Wikipedia page, blog post, article, etc.)\n",
    "- Save the page as a PDF file (you can use your browser’s print-to-PDF feature)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "503e350d29a2a468"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "49aa94dce815bc8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8d3b2dd1ab3cb014"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# **Part 2: Repeat the Lab with Your Own Knowledge Base + RAG Tuning**\n",
    "\n",
    "## **Goal:**\n",
    "\n",
    "Practice building a **RAG pipeline** and explore how **chunk size** and **chunk overlap** affect the quality of LLM answers to different questions.\n",
    "\n",
    "---\n",
    "\n",
    "## **What You Need to Do:**\n",
    "\n",
    "1. **Repeat the Lab Using Your PDF Knowledge Base**\n",
    "\n",
    "   * Use the PDF file you selected and loaded in Part 1.\n",
    "\n",
    "2. **Create 3 Different Questions**\n",
    "\n",
    "   * Design **three meaningful, specific questions** based on your topic.\n",
    "   * Each question must be clearly related to the content of your PDF.\n",
    "\n",
    "3. **Run RAG for Each Question with 3 Different Settings:**\n",
    "   For each question:\n",
    "\n",
    "   * Run the RAG pipeline **three times** using different settings for:\n",
    "\n",
    "     * `chunk_size` (e.g., 100, 300, 500)\n",
    "     * `chunk_overlap` (e.g., 0, 20, 50, 100)\n",
    "   * This means you will run a total of **9 tests** (3 questions × 3 settings each).\n",
    "\n",
    "\n",
    "4. **Answer Each Question Using an LLM**\n",
    "\n",
    "   * Use the loaded chunks and a retriever to find relevant parts.\n",
    "   * Pass the retrieved context to the LLM and generate an answer.\n",
    "   * You can use similar tools as we used in the Lab\n",
    "\n",
    "5. **Explain Your Results**\n",
    "   For each of the 3 questions:\n",
    "\n",
    "   * Write a short **description of the question** and **why you chose it**.\n",
    "   * **Compare the answers** you got using different settings.\n",
    "   * Reflect on:\n",
    "\n",
    "     * How answer quality changed with different `chunk_size` and `chunk_overlap`\n",
    "     * Which setting gave the most useful or accurate result\n",
    "     * Why you think it performed better/worse\n",
    "\n",
    "---\n",
    "\n",
    "## **Deliverables:**\n",
    "\n",
    "* Python code used for RAG pipeline (with different chunking settings)\n",
    "* PDF file from Part 1\n",
    "* A JSON file named rag_report_last_name_name_id.json containing your results:\n",
    "\n",
    "  * 3 questions with explanations\n",
    "  * Generated answers for each setting\n",
    "  * Comparison and reflection on the results\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e7acc5cf37e6117"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "39be2ec371b6c408"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b150f17b07735b15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Template for your resulting json file with report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8db167ca254763a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "your_results_dict = {\n",
    "  \"topic\": \"Write the title or theme of your chosen topic here (e.g., 'AI Developments in 2024')\",\n",
    "  \"question\":\"write your question you used to test your topic with plain model without RAG\",\n",
    "  \"answer\":\"initial answer of the model without using RAG\",\n",
    "  \"rag\": [\n",
    "    {\n",
    "      \"question\": \"Write your first custom question here\",\n",
    "      \"reason\": \"Briefly explain why this question is important or relevant to your topic\",\n",
    "      \"experiments\": [\n",
    "        {\n",
    "          \"chunk_size\": \"Enter an integer value for chunk size (e.g., 300, 500, etc.)\",\n",
    "          \"chunk_overlap\": \"Enter an integer value for chunk overlap (e.g., 0, 100, etc.)\",\n",
    "          \"answer\": \"Paste here the answer generated by the LLM for this setting\",\n",
    "          \"reflection\": \"Write your analysis of the answer: Was it accurate, detailed, too short, off-topic, etc.?\"\n",
    "        },\n",
    "        {\n",
    "          \"chunk_size\": \"Another chunk size value for testing (should differ from above)\",\n",
    "          \"chunk_overlap\": \"Corresponding chunk overlap value for this test\",\n",
    "          \"answer\": \"LLM-generated answer for the second test\",\n",
    "          \"reflection\": \"Compare this result with the previous one—was it better or worse? Why?\"\n",
    "        },\n",
    "        {\n",
    "          \"chunk_size\": \"Third chunk size value for testing\",\n",
    "          \"chunk_overlap\": \"Third chunk overlap value for testing\",\n",
    "          \"answer\": \"LLM-generated answer for the third test\",\n",
    "          \"reflection\": \"Describe how this answer compares with the others and what you learned\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"Write your second custom question here\",\n",
    "      \"reason\": \"Explain why this question is meaningful to your topic\",\n",
    "      \"experiments\": [\n",
    "        {\n",
    "          \"chunk_size\": \"Integer value for chunk size\",\n",
    "          \"chunk_overlap\": \"Integer value for chunk overlap\",\n",
    "          \"answer\": \"LLM response using this setting\",\n",
    "          \"reflection\": \"Your evaluation of the answer quality with these parameters\"\n",
    "        },\n",
    "        {\n",
    "          \"chunk_size\": \"Another chunk size\",\n",
    "          \"chunk_overlap\": \"Another chunk overlap\",\n",
    "          \"answer\": \"LLM response for second configuration\",\n",
    "          \"reflection\": \"How did the result change? Why might that be?\"\n",
    "        },\n",
    "        {\n",
    "          \"chunk_size\": \"Third chunk size\",\n",
    "          \"chunk_overlap\": \"Third chunk overlap\",\n",
    "          \"answer\": \"Final LLM result for this question\",\n",
    "          \"reflection\": \"Summarize your findings from all three tests for this question\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"Write your third custom question here\",\n",
    "      \"reason\": \"Explain why this question is useful or interesting\",\n",
    "      \"experiments\": [\n",
    "        {\n",
    "          \"chunk_size\": \"Integer chunk size value\",\n",
    "          \"chunk_overlap\": \"Integer chunk overlap value\",\n",
    "          \"answer\": \"Answer generated with this config\",\n",
    "          \"reflection\": \"How well did it perform? Was it relevant?\"\n",
    "        },\n",
    "        {\n",
    "          \"chunk_size\": \"Different chunk size value\",\n",
    "          \"chunk_overlap\": \"Different chunk overlap value\",\n",
    "          \"answer\": \"Second test answer\",\n",
    "          \"reflection\": \"Comparison with first result\"\n",
    "        },\n",
    "        {\n",
    "          \"chunk_size\": \"Third chunk size\",\n",
    "          \"chunk_overlap\": \"Third chunk overlap\",\n",
    "          \"answer\": \"Third LLM-generated answer\",\n",
    "          \"reflection\": \"Overall evaluation of the test results\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-28T22:36:51.042915Z",
     "start_time": "2025-05-28T22:36:51.040385Z"
    }
   },
   "id": "eef0bdf7e079c909"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"rag_report_Arkadiusz_Modzelewski_29580.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(your_results_dict, f, indent=2, ensure_ascii=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-28T22:36:51.344609Z",
     "start_time": "2025-05-28T22:36:51.341408Z"
    }
   },
   "id": "50084e2063ae7fb0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
