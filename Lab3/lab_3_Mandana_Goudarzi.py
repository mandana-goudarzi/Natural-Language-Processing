# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VLCDA5y3cBxzP_fMdNfrHE-C8NWdrsFz
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import matplotlib.pyplot as plt
import math
import pandas as pd
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import transformers
import torch
import math

!git clone https://github.com/elenipapadopulos/NLP_LAB3_Datasets.git

df = pd.read_csv("/content/NLP_LAB3_Datasets/typo_dataset1.csv")

device =  "cuda:0" if torch.cuda.is_available() else "cpu"

model = "gpt2"

tokenizer = GPT2Tokenizer.from_pretrained(model)
model = GPT2LMHeadModel.from_pretrained(model, pad_token_id = tokenizer.eos_token_id).to(device)

tokenizer.pad_token = tokenizer.eos_token # set the pad to eos_token
model.config.pad_token_id = tokenizer.pad_token_id

model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

bos_token = "<s>"
tokenizer.add_tokens([bos_token])
tokenizer.bos_token = bos_token
model.resize_token_embeddings(len(tokenizer))

def get_token_logprobs(sentence):

    ## tokenize the sentence, compute the output and retrieve logits
    inputs = tokenizer(sentence, return_tensors='pt').to(device)
    with torch.no_grad():
      logits = model(**inputs).logits.squeeze()[:-1]
    #
    #
    #

    ## remember: logits.shape is (1, sen_len, model_size])
    ## remove the logits relative to the last token: we are not interested in next token generation
    ## expected shape: (sen_len, model_size) (suggestion: use squeeze))
    #

    ## retrieve the indices (input_ids) of the sentence
    ## hint: remove the input_id relative to the bos
    ## expected shape: (sen_len)
    indices = inputs["input_ids"].squeeze(0)[1:]

    ## compute log-probabilities
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

    ## retrieve the probabilities of the tokens
    token_logprobs = log_probs[range(len(indices)), indices]

    ## convert input ids to tokens to obtain a list of tokens
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"].squeeze(0))

    return tokens, token_logprobs.tolist()

def get_cumulative_token_logprobs(sentence):

    tokens, token_logprobs = get_token_logprobs(sentence)
    cumulative_logprobs = []
    current_running_sum = 0
    for token, logprob in zip(tokens, token_logprobs) :
      current_running_sum += logprob
      cumulative_logprobs.append((token, current_running_sum))



    # write your code here

    return cumulative_logprobs

def detect_typos(sentence, threshold=5):
    word_logprobs = get_cumulative_token_logprobs(sentence)
    probs = [p for _, p in word_logprobs]

    for i in range(1, len(probs)):
        drop = abs(probs[i] - probs[i-1])
        if drop > threshold:
            label = 0
            break
    else:
        label = 1

    return label

correct = 0

for i, row in df.iterrows():
  sentence = row["text"]
  label = row["label"]
  pred = detect_typos(sentence, threshold=13.0)

  if label == pred:
    correct += 1

print(f"Accuracy: {correct/len(df)}")

"""## Observations:
- The model tends to assign lower probabilities to rare or unexpected words which is helpful for typo detection.

- In many cases, typos like misspellings or wrong word forms caused a noticeable drop in log-probabilities, which the threshold caught effectively.

## Limitations:
- Not all typos cause a sharp enough drop. For instance, homophones like their/there or your/you're might still get high probabilities, since both are grammatically and semantically common.

- The model can be sensitive to rare but correct words (like named entities), flagging them as typos.

- The threshold value is static and might not generalize well across sentences of different lengths or types.

## Suggestions for improvement:
Instead of relying only on the drop in log-probabilities, we could also look at other language signals, like:

- part-of-speech (POS) mismatches (e.g., using a verb where a noun is expected),

- grammar rules

- and simple spelling checks (e.g., how different a word is from the correct one).

Another idea is to train a small model that combines different features like log-probs, how common the word is, or word embeddings to predict typos more accurately.

We could also use entropy to detect tokens the model is unsure about not just sudden drops.
"""

